{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKQvFAJpAGFZ",
        "outputId": "876af009-982f-4884-c433-eb2b0ced257d"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_2VXIZqE404",
        "outputId": "6f9c3178-c713-409d-d895-dc151e55b139"
      },
      "source": [
        "!pip install picos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting picos\n",
            "  Downloading PICOS-2.2.55.tar.gz (320 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 39.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 71 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 81 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 92 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 102 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 112 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 122 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 133 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 143 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 153 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 163 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 174 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 184 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 194 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 204 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 215 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 225 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 235 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 245 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 256 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 266 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 276 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 286 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 296 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 307 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 317 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 320 kB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (from picos) (1.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from picos) (1.19.5)\n",
            "Building wheels for collected packages: picos\n",
            "  Building wheel for picos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for picos: filename=PICOS-2.2.55-py3-none-any.whl size=448519 sha256=d90cd61d298bc61d1ffc51694e03165a141723ba93ae6fd6886f678f8e637971\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/5f/db/daac50b3d79b174bb82421b89e5724512468df6982695a4c70\n",
            "Successfully built picos\n",
            "Installing collected packages: picos\n",
            "Successfully installed picos-2.2.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upEDgX0LEuFz"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# Copyright (c) 2014 - 2018  Mateo Rojas-Carulla  [mrojascarulla@gmail.com]\n",
        "# All rights reserved.  See the file COPYING for license terms.\n",
        "\n",
        "import autograd.numpy as np_aut\n",
        "import autograd\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from scipy import io\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from matplotlib import pyplot as pl\n",
        "from matplotlib import rc\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import picos as pic\n",
        "from cvxopt import matrix, solvers\n",
        "\n",
        "def split_train_valid(x, y, n_ex, valid_split=0.1):\n",
        "    n_ex_cum = np.append(0, np.cumsum(n_ex))\n",
        "    n_ex_train, n_ex_valid = [], []\n",
        "    train_x, train_y, valid_x, valid_y = [], [], [], []\n",
        "\n",
        "    for i in range(len(n_ex)):\n",
        "        n_train_task = int((1 - valid_split) * n_ex[i])\n",
        "        train_x.append(x[n_ex_cum[i]:n_ex_cum[i] + n_train_task])\n",
        "        train_y.append(y[n_ex_cum[i]:n_ex_cum[i] + n_train_task])\n",
        "\n",
        "        valid_x.append(x[n_ex_cum[i] + n_train_task:n_ex_cum[i + 1]])\n",
        "        valid_y.append(y[n_ex_cum[i] + n_train_task:n_ex_cum[i + 1]])\n",
        "        \n",
        "        n_ex_train.append(n_train_task)\n",
        "        n_ex_valid.append(n_ex[i] - n_train_task)\n",
        "\n",
        "    train_x = np.concatenate(train_x, 0)\n",
        "    valid_x = np.concatenate(valid_x, 0)\n",
        "    train_y = np.concatenate(train_y, 0)\n",
        "    valid_y = np.concatenate(valid_y, 0)\n",
        "\n",
        "    n_ex_train = np.array(n_ex_train)\n",
        "    n_ex_valid = np.array(n_ex_valid)\n",
        "\n",
        "    return train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid\n",
        "\n",
        "\n",
        "\n",
        "def np_getDistances(x,y):\n",
        "    K = (x[:,:, np.newaxis] - y.T)\n",
        "    return np.linalg.norm(K,axis = 1)\n",
        "\n",
        "    \n",
        "#Select top 11 predictors from Lasso\n",
        "def lasso_alpha_search_synt(X,Y):\n",
        "\n",
        "    exit_loop = False\n",
        "    alpha_lasso = 0.2\n",
        "    step = 0.02\n",
        "    num_iters = 1000\n",
        "    count = 0\n",
        "    n = 11\n",
        "\n",
        "    while(not exit_loop and count < num_iters):\n",
        "            count = count + 1\n",
        "\n",
        "            regr = linear_model.Lasso(alpha = alpha_lasso)\n",
        "            regr.fit(X,Y.flatten())\n",
        "            zeros =  np.where(np.abs(regr.coef_) < 0.00000000001)\n",
        "\n",
        "            nonzeros = X.shape[1]-zeros[0].shape[0]\n",
        "\n",
        "            if(nonzeros >= n and nonzeros<n+1):\n",
        "                    exit_loop = True\n",
        "            if nonzeros<n:\n",
        "                    alpha_lasso -= step\n",
        "            else:\n",
        "                    step /= 2\n",
        "                    alpha_lasso += step\n",
        "\n",
        "\n",
        "    mask = np.ones(X.shape[1],dtype = bool)\n",
        "    mask[zeros] = False\n",
        "    genes = []\n",
        "    index_mask = np.where(mask == True)[0]\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "#Given a number of training tasks, the total number of examples and the number per task, return a boolean mask (for SMTL)\n",
        "def mask_training_tasks(n_tasks,n_s,n_tot,n_pred):\n",
        "    mask = np.zeros((n_tot,n_pred),dtype = bool)\n",
        "    n_each = n_tot/n_tasks\n",
        "    for t in range(n_tasks):\n",
        "        mask[t*n_each:t*n_each+n_s,:] = True\n",
        "    return mask\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Run and SDP to find a feasible solution when optimising (4)\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "def find_init_sol(Cov_ctr,s_size, n_size):\n",
        "\n",
        "    shape = Cov_ctr.shape[0]\n",
        "    sdp = pic.Problem()\n",
        "    X = sdp.add_variable('X', (shape,shape), vtype='symmetric')\n",
        "    Cov_ctr = pic.new_param('M', matrix(Cov_ctr))\n",
        "\n",
        "    #Matrix has to be spd\n",
        "    sdp.add_constraint(X>>0)\n",
        "    for i in range(shape):\n",
        "        for j in range(i+1):\n",
        "            if i==shape-1 and j>=s_size and j<shape-1:\n",
        "                continue\n",
        "            else:\n",
        "                sdp.add_constraint(X[i,j] == Cov_ctr[i,j])\n",
        "\n",
        "    sdp.set_objective('min',0*sum(X))\n",
        "    sol = sdp.solve(solver = 'cvxopt', verbose = False)\n",
        "    X = np.array(X.value)\n",
        "\n",
        "    return X\n",
        " \n",
        "def find_init_sol_b(Cov_ctr, fix):\n",
        "\n",
        "    shape = Cov_ctr.shape[0]\n",
        "    sdp = pic.Problem()\n",
        "    X = sdp.add_variable('X', (shape,shape), vtype='symmetric')\n",
        "    Cov_ctr = pic.new_param('M', matrix(Cov_ctr))\n",
        "\n",
        "    #Matrix has to be spd\n",
        "    sdp.add_constraint(X>>0)\n",
        "\n",
        "    for i in range(shape-fix, shape):\n",
        "        for j in range(shape-fix, shape):\n",
        "          sdp.add_constraint(X[i,j] == Cov_ctr[i,j])\n",
        "\n",
        "    sdp.set_objective('min', 0*sum(X))\n",
        "    sol = sdp.solve(solver = 'cvxopt', verbose = True)\n",
        "    X = np.array(X.value)\n",
        "\n",
        "    return X\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "# Compute beta for the naive plug-in estimator\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "def compute_beta_naive(X,X_l,Y_l,S,alpha,eps,numCauses, X_tr = 0, Y_tr = 0):\n",
        "\n",
        "    ns_l = X.shape[0]\n",
        "    ns_s = Y_l.size\n",
        "\n",
        "    \n",
        "    numEffects = X.shape[1]-numCauses\n",
        "    numPredictors = X.shape[1]\n",
        "    \n",
        "    cov_x = 1./ns_l*np.dot(X.T,X)\n",
        "    cov_xs = cov_x[0:numCauses,0:numCauses]\n",
        "    \n",
        "    cov_ys = np.dot(cov_xs,alpha)\n",
        "\n",
        "    cov_yn = 1./ns_s*np.dot(X_l[:,numCauses:].T,Y_l)\n",
        "    cov_xy = np.append(cov_ys, cov_yn)[:,np.newaxis]\n",
        "    \n",
        "    cy = np.dot(alpha[np.newaxis,:],\n",
        "                np.dot(cov_xs,alpha[:,np.newaxis])) + eps**2\n",
        "        \n",
        "    cov_y = np.concatenate([cov_xy,cy]).T\n",
        "        \n",
        "    temp = np.append(cov_x,cov_xy.T,axis=0)\n",
        "    cov = np.append(temp,cov_y.T,axis=1)\n",
        "\n",
        "    cov_x = cov[0:-1,0:-1]\n",
        "    cov_xy = cov[-1,0:-1][:,np.newaxis]\n",
        "\n",
        "    beta_est =  np.dot(np.linalg.inv(cov_x),cov_xy)\n",
        "\n",
        "    return beta_est\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "# Maximize (4) and return beta\n",
        "#-------------------------------------------------------------------\n",
        "\n",
        "def compute_beta_mtl(X,X_l,Y_l,S,alpha,eps,numCauses,X_tr = 0,\n",
        "                     Y_tr = 0, opti_alpha = False,\n",
        "                     true_cov = None):\n",
        "\n",
        "    ns_l = X.shape[0]\n",
        "    ns_s = Y_l.size\n",
        "    \n",
        "    numEffects = X.shape[1]-numCauses\n",
        "    numPredictors = X.shape[1]\n",
        "\n",
        "    if true_cov ==None:\n",
        "        cov_x = np.cov(X.T)\n",
        "    else:\n",
        "        cov_x = true_cov[0:-1,0:-1]\n",
        "        \n",
        "\n",
        "    if numCauses == numPredictors:\n",
        "        cov_xs = cov_x[0:numCauses,0:numCauses]\n",
        "        cov_ys = np.dot(cov_xs,alpha)\n",
        "        cov_xy = cov_ys[:,np.newaxis]\n",
        "        cy = np.dot(alpha[np.newaxis,:],\n",
        "                np.dot(cov_xs,alpha[:,np.newaxis])) + eps**2\n",
        "\n",
        "        cov_y = np.concatenate([cov_xy,cy]).T\n",
        "        temp = np.append(cov_x,cov_xy.T,axis=0)\n",
        "        cov = np.append(temp,cov_y.T,axis=1)\n",
        "\n",
        "        cov_x = cov[0:-1,0:-1]\n",
        "        cov_xy = cov[-1,0:-1][:,np.newaxis]\n",
        "        \n",
        "        beta_est =  np.dot(np.linalg.inv(cov_x),cov_xy)\n",
        "        return beta_est\n",
        "\n",
        "    \n",
        "    elif numCauses ==0:\n",
        "        cov_xy = 1./ns_s*np.dot(X_l[:,numCauses:].T,Y_l)\n",
        "        cov_yn = cov_xy\n",
        "        cy = np.array([eps**2])[:,np.newaxis]\n",
        "        \n",
        "    else:\n",
        "        cov_xs = cov_x[0:numCauses,0:numCauses]\n",
        "        cov_ys = np.dot(cov_xs,alpha)\n",
        "        cov_yn = 1./ns_s*np.dot(X_l[:,numCauses:].T,Y_l)\n",
        "\n",
        "        cov_xy = np.append(cov_ys, cov_yn)[:,np.newaxis]\n",
        "        cy = np.dot(alpha[np.newaxis,:],\n",
        "                    np.dot(cov_xs,alpha[:,np.newaxis])) + eps**2\n",
        "\n",
        "    \n",
        "    cov_y = np.concatenate([cov_xy,cy]).T\n",
        "    temp = np.append(cov_x,cov_xy.T,axis=0)\n",
        "    x = cov_yn\n",
        "\n",
        "    M = np.append(temp,cov_y.T,axis=1)\n",
        "    \n",
        "    def logl_chol(u):\n",
        "        \n",
        "        Mat = M\n",
        "\n",
        "        Mat[-1,numCauses:-1] = u\n",
        "        Mat[numCauses:-1,-1] = u.T\n",
        "        \n",
        "        try:\n",
        "            M_inv = np_aut.linalg.inv(Mat)\n",
        "            det = np_aut.linalg.det(M_inv)\n",
        "            if np_aut.isnan(det) or det<0:\n",
        "                log_det = -1e5\n",
        "            else: log_det = np_aut.log(det)\n",
        "            \n",
        "            ret = np_aut.trace(np_aut.dot(M_inv,S)) - log_det\n",
        "            \n",
        "        except Exception:\n",
        "            ret = 1e5\n",
        "    \n",
        "        return ret\n",
        "    \n",
        "        \n",
        "    cov= find_init_sol(M,numCauses,numEffects)\n",
        "    M[-1,numCauses:-1] = cov[-1,numCauses:-1]\n",
        "    \n",
        "    x_init = M[-1,numCauses:-1]\n",
        "    tol= 1e-10\n",
        "    res = sc.optimize.fmin(logl_chol,x_init,\n",
        "                            xtol = tol,\n",
        "                            ftol = tol,\n",
        "                            maxiter = 1e5,\n",
        "                            maxfun = 3e5,\n",
        "                            disp = False)\n",
        "\n",
        "    M[-1,numCauses:-1] = res\n",
        "    M[numCauses:-1,-1] = res.T\n",
        "    \n",
        "    cov[-1,numCauses:-1] = M[-1,numCauses:-1]\n",
        "    cov[numCauses:-1,-1] = M[numCauses:-1,-1]    \n",
        "    cov_x = cov[0:-1,0:-1]\n",
        "    cov_xy = cov[-1,0:-1][:,np.newaxis]\n",
        "\n",
        "    beta_est =  np.dot(np.linalg.inv(cov_x),cov_xy)\n",
        "\n",
        "    return beta_est\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "#Return MTL coefficient for both the naive and the approach maximizing (4)\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def error_naive_beta(train_x, train_y,\n",
        "                   X_lab,Y_lab,\n",
        "                   X_ul,test_x,test_y,\n",
        "                   subset,cov,n,p, alpha=np.zeros(1), eps=0,min_el = 0):\n",
        "\n",
        "    if eps==0:\n",
        "        train_x_all = np.append(train_x, X_lab, axis=0)\n",
        "        train_y_all = np.append(train_y, Y_lab, axis=0)\n",
        "        regr = linear_model.LinearRegression()\n",
        "        regr.fit(train_x_all[:,subset],train_y_all)\n",
        "        pred = regr.predict(train_x_all[:,subset])\n",
        "        alpha = regr.coef_\n",
        "        eps = np.std(train_y_all-pred)\n",
        "\n",
        "    else:\n",
        "        alpha = alpha\n",
        "        eps = eps\n",
        "    \n",
        "    mask = np.ones(p, dtype = bool)\n",
        "\n",
        "    mask[subset] = False\n",
        "\n",
        "    s_size = subset.size\n",
        "\n",
        "    X_lab_perm = np.concatenate([X_lab[:,subset],X_lab[:,mask]],axis=1)\n",
        "    X_ul_perm = np.concatenate([X_ul[:,subset],X_ul[:,mask]],axis=1)\n",
        "\n",
        "    cov = np.concatenate([X_lab_perm, Y_lab], axis=1)\n",
        "    cov = 1./n*np.dot(cov.T,cov)\n",
        "                             \n",
        "    beta = compute_beta_naive(np.append(X_lab_perm, X_ul_perm,axis=0),\n",
        "                            X_lab_perm,\n",
        "                            Y_lab,\n",
        "                            cov,\n",
        "                            alpha.flatten(),\n",
        "                            eps,\n",
        "                            s_size,\n",
        "                            train_x,\n",
        "                            train_y)\n",
        "\n",
        "    bs = beta[0:s_size].flatten()\n",
        "    bn  = beta[s_size:].flatten()\n",
        "    \n",
        "    if min_el != 0:\n",
        "        n = min_el\n",
        "\n",
        "    pred_test = np.sum(bs*test_x[:,subset],axis=1) + np.sum(bn*test_x[:,mask],1)\n",
        "    pred_test = pred_test[:,np.newaxis]\n",
        "    \n",
        "    mse_test =np.mean((pred_test-test_y)**2)\n",
        "    if subset.size > 0 and subset.size<p:\n",
        "        b = np.zeros(p)\n",
        "        b[subset] = bs\n",
        "        b[mask] = bn\n",
        "    else:\n",
        "        b = beta.flatten()\n",
        "\n",
        "    return mse_test, b[:,np.newaxis]\n",
        "\n",
        "\n",
        "def error_mle_beta(train_x, train_y,\n",
        "                   X_lab,Y_lab,\n",
        "                   X_ul,test_x,test_y,\n",
        "                   subset,cov,n,p, alpha=np.zeros(1), eps=0,min_el = 0,\n",
        "                   opti_alpha = False,\n",
        "                   true_cov = None):\n",
        "\n",
        "    if eps==0:\n",
        "        train_x_all = np.append(train_x, X_lab, axis=0)\n",
        "        train_y_all = np.append(train_y, Y_lab, axis=0)\n",
        "        \n",
        "        if subset.size > 0:\n",
        "            regr = linear_model.LinearRegression()\n",
        "            regr.fit(train_x_all[:,subset],train_y_all)\n",
        "            pred = regr.predict(train_x_all[:,subset])\n",
        "            alpha = regr.coef_.flatten()\n",
        "            eps = np.std(train_y_all-pred)\n",
        "\n",
        "        else:\n",
        "            alpha = np.zeros(1)\n",
        "            eps = np.std(train_y_all)\n",
        "\n",
        "    else:\n",
        "        alpha = alpha\n",
        "        eps = eps\n",
        "\n",
        "    mask = np.ones(p, dtype = bool)\n",
        "    if subset.size>0:\n",
        "        mask[subset] = False\n",
        "\n",
        "    s_size = subset.size\n",
        "    if s_size > 0:\n",
        "        X_lab_perm = np.concatenate([X_lab[:,subset],X_lab[:,mask]],axis=1)\n",
        "        X_ul_perm = np.concatenate([X_ul[:,subset],X_ul[:,mask]],axis=1)\n",
        "    else:\n",
        "        X_lab_perm = X_lab\n",
        "        X_ul_perm = X_ul\n",
        "\n",
        "    cov = np.concatenate([X_lab_perm, Y_lab], axis=1)\n",
        "    cov = np.cov(cov.T)\n",
        "    \n",
        "    beta = compute_beta_mtl(np.append(X_lab_perm, X_ul_perm,axis=0),\n",
        "                            X_lab_perm,\n",
        "                            Y_lab,\n",
        "                            cov,\n",
        "                            alpha.flatten(),\n",
        "                            eps,\n",
        "                            s_size,\n",
        "                            train_x,\n",
        "                            train_y,\n",
        "                            opti_alpha,\n",
        "                            true_cov)\n",
        "\n",
        "    bs = beta[0:s_size].flatten()\n",
        "    bn  = beta[s_size:].flatten()\n",
        "    \n",
        "    #if min_el != 0:\n",
        "        #n = min_el\n",
        "\n",
        "    if subset.size>0:\n",
        "        pred_test = np.sum(bs*test_x[:,subset],axis=1)+ np.sum(bn*test_x[:,mask],1)\n",
        "    else: pred_test = np.sum(bn*test_x[:,mask],1)\n",
        "    pred_test = pred_test[:,np.newaxis]\n",
        "    \n",
        "    mse_test =np.mean((pred_test-test_y)**2)\n",
        "\n",
        "    if subset.size > 0 and subset.size<p:\n",
        "        b = np.zeros(p)\n",
        "        b[subset] = bs\n",
        "        b[mask] = bn\n",
        "    else:\n",
        "        b = beta.flatten()\n",
        "\n",
        "    return mse_test, b[:,np.newaxis]\n",
        "\n",
        "    \n",
        "def error_mle_beta_cv(train_x, train_y,\n",
        "                   X_lab_all, Y_lab_all,\n",
        "                   X_ul,\n",
        "                   subset_list,cov,n,p, \n",
        "                   alpha=np.zeros(1), eps=0,min_el = 0,\n",
        "                   opti_alpha = False,\n",
        "                   true_cov = None):\n",
        "\n",
        "    scores = []\n",
        "    fold = 5\n",
        "\n",
        "    kf = KFold(X_lab_all.shape[0], n_folds = fold)\n",
        "\n",
        "    for subset in subset_list:\n",
        "      scores_temp = []\n",
        "\n",
        "      for train, test in kf:\n",
        "        X_lab = X_lab_all[train]\n",
        "        Y_lab = Y_lab_all[train]\n",
        "        n = X_lab.shape[0]\n",
        "\n",
        "        if eps==0:\n",
        "            train_x_all = np.append(train_x, X_lab, axis=0)\n",
        "            train_y_all = np.append(train_y, Y_lab, axis=0)\n",
        "            \n",
        "            if subset.size > 0:\n",
        "                regr = linear_model.LinearRegression()\n",
        "                regr.fit(train_x_all[:,subset],train_y_all)\n",
        "                pred = regr.predict(train_x_all[:,subset])\n",
        "                alpha = regr.coef_.flatten()\n",
        "                eps = np.std(train_y_all-pred)\n",
        "\n",
        "            else:\n",
        "                alpha = np.zeros(1)\n",
        "                eps = np.std(train_y_all)\n",
        "\n",
        "        else:\n",
        "            alpha = alpha\n",
        "            eps = eps\n",
        "\n",
        "        mask = np.ones(p, dtype = bool)\n",
        "        if subset.size>0:\n",
        "            mask[subset] = False\n",
        "\n",
        "    \n",
        "        s_size = subset.size\n",
        "        if s_size > 0:\n",
        "            X_lab_perm = np.concatenate([X_lab[:,subset],X_lab[:,mask]],axis=1)\n",
        "            X_ul_perm = np.concatenate([X_ul[:,subset],X_ul[:,mask]],axis=1)\n",
        "        else:\n",
        "            X_lab_perm = X_lab\n",
        "            X_ul_perm = X_ul\n",
        "\n",
        "        cov = np.concatenate([X_lab_perm, Y_lab], axis=1)\n",
        "        cov = np.cov(cov.T)\n",
        "        \n",
        "        beta = compute_beta_mtl(np.append(X_lab_perm, X_ul_perm,axis=0),\n",
        "                                X_lab_perm,\n",
        "                                Y_lab,\n",
        "                                cov,\n",
        "                                alpha.flatten(),\n",
        "                                eps,\n",
        "                                s_size,\n",
        "                                train_x,\n",
        "                                train_y,\n",
        "                                opti_alpha,\n",
        "                                true_cov)\n",
        "\n",
        "        bs = beta[0:s_size].flatten()\n",
        "        bn  = beta[s_size:].flatten()\n",
        "        \n",
        "        test_x, test_y = X_lab_all[test], Y_lab_all[test]\n",
        "\n",
        "        if subset.size>0:\n",
        "            pred_test = np.sum(bs*test_x[:,subset],axis=1)+ np.sum(bn*test_x[:,mask],1)\n",
        "        else: pred_test = np.sum(bn*test_x[:,mask],1)\n",
        "        pred_test = pred_test[:,np.newaxis]\n",
        "        \n",
        "        mse_test =np.mean((pred_test-test_y)**2)\n",
        "\n",
        "        if subset.size > 0 and subset.size<p:\n",
        "            b = np.zeros(p)\n",
        "            b[subset] = bs\n",
        "            b[mask] = bn\n",
        "        else:\n",
        "            b = beta.flatten()\n",
        "        scores_temp.append(mse_test)\n",
        "        eps = 0\n",
        "      scores.append(np.mean(scores_temp))\n",
        "\n",
        "    return subset_list[np.argmin(scores)]\n",
        "\n",
        "\n",
        "def np_getDistances(x,y):\n",
        "\tK = (x[:,:, np.newaxis] - y.T)\n",
        "\treturn np.linalg.norm(K,axis = 1)\n",
        "\n",
        "def np_gaussian_kernel(x,y, beta=0.1):\n",
        "    K = np_outer_substract(x,y)\n",
        "    return np.exp( -beta * np.linalg.norm(K, axis=1))\n",
        "\n",
        "def mat_hsic(X,nEx):\n",
        "\n",
        "\tnExCum = np.cumsum(nEx)\n",
        "\tdomains = np.zeros((np.sum(nEx),np.sum(nEx)))\n",
        "\tcurrentIndex = 0\n",
        "\n",
        "\tfor i in range(nEx.size):\n",
        "\n",
        "\t\tdomains[currentIndex:nExCum[i], currentIndex:nExCum[i]] = np.ones((nEx[i], nEx[i]))\n",
        "\t\tcurrentIndex = nExCum[i]\n",
        "\n",
        "\treturn domains\n",
        "\n",
        "def numpy_GetKernelMat(X,sX):\n",
        "\n",
        "\tKernel = (X[:,:, np.newaxis] - X.T).T\n",
        "\tKernel = np.exp( -1./(2*sX) * np.linalg.norm(Kernel, axis=1))\n",
        "\n",
        "\treturn Kernel\n",
        "\n",
        "def numpy_HsicGammaTest(X,Y, sigmaX, sigmaY, DomKer = 0):\n",
        "\n",
        "\tn = X.T.shape[1]\n",
        "\n",
        "\tKernelX = numpy_GetKernelMat(X,sigmaX)\n",
        "\n",
        "\tKernelY = DomKer\n",
        "\n",
        "\tcoef = 1./n\n",
        "\tHSIC = coef**2*np.sum(KernelX*KernelY) + coef**4*np.sum(\n",
        "                KernelX)*np.sum(KernelY) - 2*coef**3*np.sum(np.sum(KernelX,axis=1)*np.sum(KernelY, axis=1))\n",
        "\t\n",
        "\t#Get sums of Kernels\n",
        "\tKXsum = np.sum(KernelX)\n",
        "\tKYsum = np.sum(KernelY)\n",
        "\n",
        "\t#Get stats for gamma approx\n",
        "\n",
        "\txMu = 1./(n*(n-1))*(KXsum - n)\n",
        "\tyMu = 1./(n*(n-1))*(KYsum - n)\n",
        "\tV1 = coef**2*np.sum(KernelX*KernelX) + coef**4*KXsum**2 - 2*coef**3*np.sum(np.sum(KernelX,axis=1)**2)\n",
        "\tV2 = coef**2*np.sum(KernelY*KernelY) + coef**4*KYsum**2 - 2*coef**3*np.sum(np.sum(KernelY,axis=1)**2)\n",
        "\n",
        "\tmeanH0 = (1. + xMu*yMu - xMu - yMu)/n\n",
        "\tvarH0 = 2.*(n-4)*(n-5)/(n*(n-1.)*(n-2.)*(n-3.))*V1*V2\n",
        "\n",
        "\t#Parameters of the Gamma\n",
        "\ta = meanH0**2/varH0\n",
        "\tb = n * varH0/meanH0\n",
        "\n",
        "\treturn n*HSIC, a, b\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "\treturn 1./(1+np.exp(-x))\n",
        "\n",
        "#--------------------------------------------------------\n",
        "#Process residuals for computing a Levene test\n",
        "#-------------------------------------------------------\n",
        "\n",
        "def levene_pval(Residual,nEx, numR):\n",
        "\t\n",
        "\tprev = 0 \n",
        "\tn_ex_cum = np.cumsum(nEx)\n",
        "\n",
        "\tfor j in range(numR):\n",
        "\n",
        "\t\tr1 = Residual[prev:n_ex_cum[j]]\n",
        "\n",
        "\t\tif j == 0:\n",
        "\t\t\tresidTup = (r1,)\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tresidTup = residTup + (r1,)\n",
        "\n",
        "\t\tprev = n_ex_cum[j]\n",
        "\n",
        "\treturn residTup\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Utils for Dica\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "def get_kernel_mat(x,y, sx2):\n",
        "    K = (x[:,:, np.newaxis] - y.T)\n",
        "    return np.exp(-1./(2*sx2)*np.linalg.norm(K,axis = 1)**2)\n",
        "\n",
        "def get_kernel_mat_lin(x,y, sx2):\n",
        "    K = (x[:,:, np.newaxis]*y.T)\n",
        "    return np.sum(K,axis=1)\n",
        "\n",
        "def np_getDistances(x,y):\n",
        "\tK = (x[:,:, np.newaxis] - y.T)\n",
        "\treturn np.linalg.norm(K,axis = 1)\n",
        "\n",
        "\n",
        "def get_color_dict():\n",
        "\n",
        "  colors = {\n",
        "    'pool' : 'red',\n",
        "    'lasso' : 'red',\n",
        "    'shat' : 'green',\n",
        "    'sgreed' : 'green',\n",
        "    'ssharp' : 'green',\n",
        "    'strue' : 'blue',\n",
        "    'cauid' : 'blue',\n",
        "    'causharp': 'blue',\n",
        "    'cauul' : 'blue',\n",
        "    'mean' : 'black',\n",
        "    'msda' : 'orange',\n",
        "    'mtl' : 'orange',\n",
        "    'dica' : 'orange',\n",
        "    'dom' : 'k',\n",
        "    'naive' : 'magenta'\n",
        "  }\n",
        "\n",
        "  markers = {\n",
        "    'pool' : 'o',\n",
        "    'lasso' : '^',\n",
        "    'shat' : 'o',\n",
        "    'sgreed' : '^',\n",
        "    'strue' : '^',\n",
        "    'ssharp' : 'd',\n",
        "    'cauid' : 'd',\n",
        "    'causharp' : 'h',\n",
        "    'cauul' : '^',\n",
        "    'mean' : 'o',\n",
        "    'msda' : 'o',\n",
        "    'mtl' : '^',\n",
        "    'dica' : 'd',\n",
        "    'dom' : 'o',\n",
        "    'naive' : 'o'\n",
        "  }\n",
        "\n",
        "  legends = {\n",
        "              'pool' : r'$\\beta^{CS}$',\n",
        "              'lasso' : r'$\\beta^{CS(\\hat S Lasso)}$',\n",
        "              'shat' : r'$\\beta^{CS(\\hat S)}$',\n",
        "              'ssharp' : r'$\\beta^{CS(\\hat S \\sharp)}$',\n",
        "              'strue' : r'$\\beta^{CS(cau)}$',\n",
        "              'cauid' : r'$\\beta^{CS(cau+,id)}$',\n",
        "              'causharp' : r'$\\beta^{CS(cau\\sharp)}$',\n",
        "              'cauul' : r'$\\beta^{CS(cau\\sharp UL)}$',\n",
        "              'sgreed' :r'$\\beta^{CS(\\hat{S}_{greedy})}$',\n",
        "              'mean'   : r'$\\beta^{mean}$',\n",
        "              'msda'   : r'$\\beta^{mSDA}$',\n",
        "              'mtl'   : r'$\\beta^{MTL}$',\n",
        "              'dica'   : r'$\\beta^{DICA}$',\n",
        "              'naive'   : r'$\\beta^{naive}$',\n",
        "              'dom'   : r'$\\beta^{dom}$'\n",
        "            }\n",
        "\n",
        "  return colors, markers, legends\n",
        "\n",
        "def mse(model, x, y):\n",
        "  return np.mean((model.predict(x)-y)**2)\n",
        "\n",
        "\n",
        "def intervene_on_p(l_p, sz):\n",
        "  mask = np.zeros((sz, 1), dtype = bool)\n",
        "  if len(l_p) > 0:\n",
        "    mask[l_p] = True\n",
        "  return mask\n",
        "\n",
        "\n",
        "def merge_results(f1, f2, key, direc):\n",
        "  with open(os.path.join(direc, f1), 'rb') as f:\n",
        "    r1 = pickle.load(f)\n",
        "  with open(os.path.join(direc, f2), 'rb') as f:\n",
        "    r2 = pickle.load(f)\n",
        "\n",
        "  r1['results'][key] = r2['results'][key]\n",
        "  if key not in r1['plotting'][0]:\n",
        "      r1['plotting'][0].append(key)\n",
        "\n",
        "  with open(os.path.join(direc, 'merged.pkl'),'wb') as f:\n",
        "    pickle.dump(r1, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYCp29QUVx-m"
      },
      "source": [
        "################### CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmFrylq_hT9"
      },
      "source": [
        "# ALARM DATASET\n",
        "\n",
        "# Training data 1 domain\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ANDES_DATA.csv\")\n",
        "df = df.drop(columns = df.columns[0])\n",
        "df = df.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "df = df.drop(['LVFAILURE','HYPOVOLEMIA','HISTORY','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "\n",
        "n_examples_task = 500\n",
        "n_tasks = 2\n",
        "n_test_tasks = 1\n",
        "n_predictors = 21\n",
        "n_ex = []\n",
        "\n",
        "for i in range(n_tasks):\n",
        "    n_ex.append(n_examples_task)\n",
        "\n",
        "\n",
        "n_ex = np.array(n_ex)\n",
        "\n",
        "## Taking ARTCO2 as target\n",
        "# train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df.drop([\"ARTCO2\"], axis = 1 ), df[\"ARTCO2\"], n_ex,0.5)\n",
        "\n",
        "## Taking EXPCO2 as target\n",
        "train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df.drop([\"EXPCO2\"], axis = 1 ), df[\"EXPCO2\"], n_ex,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o61VY91NaLTz"
      },
      "source": [
        "# ALARM DATASET\n",
        "\n",
        "# Training data + soft INT as  training\n",
        "le = LabelEncoder()\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ANDES_DATA.csv\")\n",
        "df = df.drop(columns = df.columns[0])\n",
        "df = df.apply(le.fit_transform)\n",
        "\n",
        "df = df.drop(['LVFAILURE','HYPOVOLEMIA','HISTORY','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/alarm_soft.csv\")\n",
        "df2 = df2.drop(columns = df2.columns[0])\n",
        "df2 = df2.apply(le.fit_transform)\n",
        "\n",
        "df2 = df2.drop(['LVFAILURE','HYPOVOLEMIA','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "df2 = df2.append(df)\n",
        "\n",
        "n_examples_task = 1000\n",
        "n_tasks = 2\n",
        "n_test_tasks = 1\n",
        "n_predictors = 21\n",
        "n_ex = []\n",
        "\n",
        "for i in range(n_tasks):\n",
        "    n_ex.append(n_examples_task)\n",
        "\n",
        "\n",
        "n_ex = np.array(n_ex)\n",
        "\n",
        "## Taking ARTCO2 as target\n",
        "# train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df2.drop([\"ARTCO2\"], axis = 1 ), df2[\"ARTCO2\"], n_ex,0.5)\n",
        "\n",
        "## Taking EXPCO2 as target\n",
        "train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df2.drop([\"EXPCO2\"], axis = 1 ), df2[\"EXPCO2\"], n_ex,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Goi0zc_xON"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "use_hsic = False \n",
        "delta = 0.05\n",
        "\n",
        "num_s = np.sum(n_ex)\n",
        "\n",
        "num_predictors = train_x.shape[1]\n",
        "best_subset = np.array([])\n",
        "best_subset_acc = np.array([])\n",
        "best_mse_overall = 1e10\n",
        "\n",
        "already_acc = False\n",
        "\n",
        "selected = np.zeros(num_predictors)\n",
        "accepted_subset = None\n",
        "\n",
        "all_sets, all_pvals = [], []\n",
        "\n",
        "\n",
        "n_iters = 10*num_predictors\n",
        "stay = 1\n",
        "\n",
        "pow_2 = np.array([2**i for i in np.arange(num_predictors)])\n",
        "\n",
        "ind = 0\n",
        "prev_stat = 0\n",
        "\n",
        "bins = []\n",
        "\n",
        "#Get numbers for the mean\n",
        "\n",
        "pred = np.mean(train_y)\n",
        "mse_current = np.mean((pred - valid_y) ** 2)\n",
        "residual = valid_y - pred\n",
        " #   residTup = np.array(residTup).flatten()\n",
        "residTup = levene_pval(residual,n_ex_valid,n_ex_valid.size)\n",
        "\n",
        "levene = sp.stats.levene(*residTup)\n",
        "\n",
        "all_sets.append(np.array([]))\n",
        "all_pvals.append(levene[1])\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if all_pvals[-1]>alpha:\n",
        "  accepted_subset = np.array([])\n",
        "\n",
        "while (stay==1):\n",
        "    \n",
        "    pvals_a = np.zeros(num_predictors)\n",
        "    statistic_a = 1e10 * np.ones(num_predictors)\n",
        "    mse_a = np.zeros(num_predictors)\n",
        "    \n",
        "    for p in range(num_predictors):\n",
        "        current_subset = np.sort(np.where(selected == 1)[0])\n",
        "        regr = linear_model.LinearRegression()\n",
        "        \n",
        "        if selected[p]==0:\n",
        "            subset_add = np.append(current_subset, p).astype(int)\n",
        "            regr.fit(train_x[:,subset_add], train_y.flatten())\n",
        "            \n",
        "            pred = regr.predict(valid_x[:,subset_add])[:,np.newaxis]\n",
        "            mse_current = np.mean((pred - valid_y)**2)\n",
        "            residual = (np.array(valid_y).flatten() - np.array(pred).flatten())\n",
        "            \n",
        "            residTup = levene_pval(residual,n_ex_valid,\n",
        "                                                 n_ex_valid.size)\n",
        "            # print(residual.shape)\n",
        "            levene = sp.stats.levene(*residTup)\n",
        "            \n",
        "            pvals_a[p] = levene[1]\n",
        "            statistic_a[p] = levene[0]\n",
        "            mse_a[p] = mse_current\n",
        "            \n",
        "            all_sets.append(subset_add)\n",
        "            all_pvals.append(levene[1])\n",
        "        \n",
        "        if selected[p] == 1:\n",
        "            acc_rem = np.copy(selected)\n",
        "            acc_rem[p] = 0\n",
        "            \n",
        "            subset_rem = np.sort(np.where(acc_rem == 1)[0])\n",
        "            \n",
        "            if subset_rem.size ==0: continue\n",
        "            \n",
        "            regr = linear_model.LinearRegression()\n",
        "            regr.fit(train_x[:,subset_rem], train_y.flatten())\n",
        "            \n",
        "            pred = regr.predict(valid_x[:,subset_rem])[:,np.newaxis]\n",
        "            mse_current = np.mean((pred - valid_y)**2)\n",
        "            residual = (np.array(valid_y).flatten() - np.array(pred).flatten())\n",
        "            \n",
        "            residTup = levene_pval(residual,n_ex_valid, \n",
        "                                                 n_ex_valid.size)\n",
        "            levene = sp.stats.levene(*residTup)\n",
        "            \n",
        "            pvals_a[p] = levene[1]\n",
        "            statistic_a[p] = levene[0]\n",
        "            mse_a[p] = mse_current\n",
        "            \n",
        "            all_sets.append(subset_rem)\n",
        "            all_pvals.append(levene[1])\n",
        "    \n",
        "    accepted = np.where(pvals_a > alpha)\n",
        "    \n",
        "    if accepted[0].size>0:\n",
        "        best_mse = np.amin(mse_a[np.where(pvals_a > alpha)])\n",
        "        already_acc = True\n",
        "        \n",
        "        selected[np.where(mse_a == best_mse)] = \\\n",
        "          (selected[np.where(mse_a == best_mse)] + 1) % 2\n",
        "        \n",
        "        accepted_subset = np.sort(np.where(selected == 1)[0])\n",
        "        binary = np.sum(pow_2 * selected)\n",
        "        \n",
        "        if (bins==binary).any():\n",
        "            stay = 0\n",
        "        bins.append(binary)\n",
        "    else:\n",
        "        best_pval_arg = np.argmin(statistic_a)\n",
        "        \n",
        "        selected[best_pval_arg] = (selected[best_pval_arg] + 1) % 2\n",
        "        binary = np.sum(pow_2 * selected)\n",
        "        \n",
        "        if (bins==binary).any():\n",
        "            stay = 0\n",
        "        bins.append(binary)\n",
        "    \n",
        "    if ind>n_iters:\n",
        "        stay = 0\n",
        "    ind += 1\n",
        "\n",
        "\n",
        "if accepted_subset is None:\n",
        "  all_pvals = np.array(all_pvals).flatten()\n",
        "  \n",
        "  max_pvals = np.argsort(all_pvals)[-1]\n",
        "  accepted_subset = np.sort(all_sets[max_pvals])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmhTFQmiIRp-",
        "outputId": "86f7eaf6-4237-4bbe-9144-fd171ee180f7"
      },
      "source": [
        "accepted_subset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  9, 12, 13, 14, 15, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSxwSBDEIb8a",
        "outputId": "5df7acf6-8ed6-4bc4-eac9-09dd2a7dc2bd"
      },
      "source": [
        "accepted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([], dtype=int64),)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "QrTFEbatb3mG",
        "outputId": "243b94a2-86b7-4080-c7c4-398223143c01"
      },
      "source": [
        "# df2.drop(['EXPCO2'], axis=1)\n",
        "df.drop(['ARTCO2'], axis=1) [ 1,  2,  3,  4, 11, 13, 14, 16, 17, 18, 20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INSUFFANESTH</th>\n",
              "      <th>ANAPHYLAXIS</th>\n",
              "      <th>TPR</th>\n",
              "      <th>EXPCO2</th>\n",
              "      <th>KINKEDTUBE</th>\n",
              "      <th>MINVOL</th>\n",
              "      <th>FIO2</th>\n",
              "      <th>PVSAT</th>\n",
              "      <th>SAO2</th>\n",
              "      <th>PAP</th>\n",
              "      <th>PULMEMBOLUS</th>\n",
              "      <th>SHUNT</th>\n",
              "      <th>INTUBATION</th>\n",
              "      <th>PRESS</th>\n",
              "      <th>DISCONNECT</th>\n",
              "      <th>MINVOLSET</th>\n",
              "      <th>VENTMACH</th>\n",
              "      <th>VENTTUBE</th>\n",
              "      <th>VENTLUNG</th>\n",
              "      <th>VENTALV</th>\n",
              "      <th>CATECHOL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     INSUFFANESTH  ANAPHYLAXIS  TPR  ...  VENTLUNG  VENTALV  CATECHOL\n",
              "0               0            0    2  ...         3        3         0\n",
              "1               0            0    2  ...         3        3         0\n",
              "2               0            0    2  ...         3        3         0\n",
              "3               0            0    2  ...         3        3         0\n",
              "4               0            0    1  ...         1        0         0\n",
              "..            ...          ...  ...  ...       ...      ...       ...\n",
              "995             0            0    0  ...         3        3         0\n",
              "996             0            0    2  ...         3        3         0\n",
              "997             0            0    1  ...         3        3         0\n",
              "998             0            0    0  ...         3        3         0\n",
              "999             0            0    0  ...         3        3         0\n",
              "\n",
              "[1000 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QOVS_KSsqjP"
      },
      "source": [
        "Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmBbOk_os7Me"
      },
      "source": [
        "# ALARM \n",
        "# # Training data 1 domain\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/ANDES_DATA.csv\")\n",
        "# df = df.drop(columns = df.columns[0])\n",
        "# df = df.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# df = df.drop(['LVFAILURE','HYPOVOLEMIA','HISTORY','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "# TRAINING + SOFT INT DATA\n",
        "le = LabelEncoder()\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ANDES_DATA.csv\")\n",
        "df = df.drop(columns = df.columns[0])\n",
        "df = df.apply(le.fit_transform)\n",
        "\n",
        "df = df.drop(['LVFAILURE','HYPOVOLEMIA','HISTORY','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/alarm_soft.csv\")\n",
        "df2 = df2.drop(columns = df2.columns[0])\n",
        "df2 = df2.apply(le.fit_transform)\n",
        "\n",
        "df2 = df2.drop(['LVFAILURE','HYPOVOLEMIA','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "df2 = df2.append(df)\n",
        "df = df2\n",
        "\n",
        "# # ECO2 TARGET\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df['KINKEDTUBE'], df['EXPCO2'],test_size=0.2)\n",
        "# X_train = X_train.values.reshape(-1,1)\n",
        "# X_test = X_test.values.reshape(-1,1)\n",
        "X_train,X_test,y_train,y_test = train_test_split(df[['INSUFFANESTH','ANAPHYLAXIS', 'TPR','KINKEDTUBE',\t'MINVOL',\t'FIO2',\t'PVSAT','PULMEMBOLUS','PRESS',\t'DISCONNECT',\t'MINVOLSET', 'VENTMACH']], df['EXPCO2'],test_size=0.2)\n",
        "\n",
        "# # ACO2 TARGET\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df['INSUFFANESTH'], df['ARTCO2'],test_size=0.2)\n",
        "# X_train = X_train.values.reshape(-1,1)\n",
        "# X_test = X_test.values.reshape(-1,1)\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df[['ANAPHYLAXIS',\t'TPR', 'EXPCO2',\t'KINKEDTUBE', 'SHUNT','PRESS','DISCONNECT','VENTMACH','VENTTUBE','VENTLUNG','CATECHOL']], df['ARTCO2'],test_size=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXDugUAGv-wM"
      },
      "source": [
        "# ALARM TESTING DATA\n",
        "\n",
        "# ACO2 TARGET\n",
        "\n",
        "# # Testing Data 8 -> Soft Int2\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/alarm_soft.csv\")\n",
        "# df = df.drop(columns = df.columns[0])\n",
        "# df = df.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# df = df.drop(['LVFAILURE','HYPOVOLEMIA','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "# # X_test = df['INSUFFANESTH']\n",
        "# # X_test = X_test.values.reshape(-1,1)\n",
        "# X_test = df[['ANAPHYLAXIS',\t'TPR', 'EXPCO2',\t'KINKEDTUBE', 'SHUNT','PRESS','DISCONNECT','VENTMACH','VENTTUBE','VENTLUNG','CATECHOL']]\n",
        "# y_test = df['ARTCO2']\n",
        "\n",
        "# # Testing Data 9 -> Hard Int2\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/alarm_hard1.csv\")\n",
        "# df = df.drop(columns = df.columns[0])\n",
        "# df = df.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# df = df.drop(['LVFAILURE','HYPOVOLEMIA','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "# # X_test = df['INSUFFANESTH']\n",
        "# # X_test = X_test.values.reshape(-1,1)\n",
        "# X_test = df[['ANAPHYLAXIS',\t'TPR', 'EXPCO2',\t'KINKEDTUBE', 'SHUNT','PRESS','DISCONNECT','VENTMACH','VENTTUBE','VENTLUNG','CATECHOL']]\n",
        "# y_test = df['ARTCO2']\n",
        "\n",
        "# ECO2 TARGET\n",
        "\n",
        "# # Testing Data 8 -> Soft Int2\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/alarm_soft.csv\")\n",
        "# df = df.drop(columns = df.columns[0])\n",
        "# df = df.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# df = df.drop(['LVFAILURE','HYPOVOLEMIA','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "# # X_test = df['KINKEDTUBE']\n",
        "# # X_test = X_test.values.reshape(-1,1)\n",
        "# X_test = df[['INSUFFANESTH','ANAPHYLAXIS', 'TPR','KINKEDTUBE',\t'MINVOL',\t'FIO2',\t'PVSAT','PULMEMBOLUS','PRESS',\t'DISCONNECT',\t'MINVOLSET', 'VENTMACH']]\n",
        "# y_test = df['EXPCO2']\n",
        "\n",
        "# Testing Data 9 -> Hard Int2\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/alarm_hard1.csv\")\n",
        "df = df.drop(columns = df.columns[0])\n",
        "df = df.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "df = df.drop(['LVFAILURE','HYPOVOLEMIA','LVEDVOLUME','ERRLOWOUTPUT','STROKEVOLUME','HR','ERRCAUTER','CVP','PCWP','HRBP','HRSAT','CO','HREKG','BP'], axis = 1)\n",
        "\n",
        "# X_test = df['KINKEDTUBE']\n",
        "# X_test = X_test.values.reshape(-1,1)\n",
        "X_test = df[['INSUFFANESTH','ANAPHYLAXIS', 'TPR','KINKEDTUBE',\t'MINVOL',\t'FIO2',\t'PVSAT','PULMEMBOLUS','PRESS',\t'DISCONNECT',\t'MINVOLSET', 'VENTMACH']]\n",
        "y_test = df['EXPCO2']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ2ZUKLEsmzd"
      },
      "source": [
        "# Adaboost Training\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "abc = AdaBoostClassifier(n_estimators = 1000,random_state=49)\n",
        "\n",
        "model1 = abc.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "wFPwYK1Ospp8",
        "outputId": "bbc2d027-4589-42ce-c9a2-3e1389fb8b06"
      },
      "source": [
        "# Adaboost Testing\n",
        "y_pred = model1.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "importances = model.feature_importances_\n",
        "import matplotlib.pyplot as plt\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "ax.barh(range(len(importances)), importances[indices])\n",
        "ax.set_yticks(range(len(importances)))\n",
        "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.586\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.74      0.78        50\n",
            "           1       0.64      0.70      0.67       545\n",
            "           2       0.59      0.43      0.50       366\n",
            "           3       0.07      0.15      0.10        39\n",
            "\n",
            "    accuracy                           0.59      1000\n",
            "   macro avg       0.53      0.51      0.51      1000\n",
            "weighted avg       0.61      0.59      0.59      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doaht9E5spw6"
      },
      "source": [
        "# RF Training\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics \n",
        "\n",
        "clf = RandomForestClassifier(n_estimators = 1000,random_state=49,class_weight='balanced')  \n",
        "model = clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "gEFNQOwhsp1Z",
        "outputId": "44d0ae42-0943-4a57-d3c5-6c4e6d0e1e38"
      },
      "source": [
        "# RF Testing\n",
        "y_pred = model.predict(X_test)\n",
        "  \n",
        "print(\"ACCURACY OF RANDOM FOREST THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "importances = model.feature_importances_\n",
        "importances\n",
        "import matplotlib.pyplot as plt\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.barh(range(len(importances)), importances[indices])\n",
        "ax.set_yticks(range(len(importances)))\n",
        "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY OF RANDOM FOREST THE MODEL:  0.749\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.76      0.72        50\n",
            "           1       0.85      0.74      0.79       545\n",
            "           2       0.70      0.84      0.76       366\n",
            "           3       0.00      0.00      0.00        39\n",
            "\n",
            "    accuracy                           0.75      1000\n",
            "   macro avg       0.56      0.59      0.57      1000\n",
            "weighted avg       0.75      0.75      0.75      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpwlObZVVeKG"
      },
      "source": [
        "############ REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFHYU8aSViz6"
      },
      "source": [
        "# ALARM DATASET\n",
        "\n",
        "# Training data 1 domain\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_train1.csv\")\n",
        "\n",
        "\n",
        "\n",
        "n_examples_task = 500\n",
        "n_tasks = 2\n",
        "n_test_tasks = 1\n",
        "n_predictors = 21\n",
        "n_ex = []\n",
        "\n",
        "for i in range(n_tasks):\n",
        "    n_ex.append(n_examples_task)\n",
        "\n",
        "\n",
        "n_ex = np.array(n_ex)\n",
        "\n",
        "## Taking ARTCO2 as target\n",
        "# train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df.drop([\"ACO2\"], axis = 1 ), df[\"ACO2\"], n_ex,0.5)\n",
        "\n",
        "## Taking EXPCO2 as target\n",
        "train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df.drop([\"ECO2\"], axis = 1 ), df[\"ECO2\"], n_ex,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJe8CyESV8F5"
      },
      "source": [
        "# ALARM DATASET\n",
        "\n",
        "# Training data + soft INT as  training\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_train1.csv\")\n",
        "\n",
        "\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_softINT2.csv\")\n",
        "\n",
        "df2 = df2.append(df)\n",
        "\n",
        "n_examples_task = 1000\n",
        "n_tasks = 2\n",
        "n_test_tasks = 1\n",
        "n_predictors = 21\n",
        "n_ex = []\n",
        "\n",
        "for i in range(n_tasks):\n",
        "    n_ex.append(n_examples_task)\n",
        "\n",
        "\n",
        "n_ex = np.array(n_ex)\n",
        "\n",
        "## Taking ARTCO2 as target\n",
        "# train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df2.drop([\"ACO2\"], axis = 1 ), df2[\"ACO2\"], n_ex,0.5)\n",
        "\n",
        "## Taking EXPCO2 as target\n",
        "train_x, train_y, valid_x, valid_y, n_ex_train, n_ex_valid = split_train_valid(df2.drop([\"ECO2\"], axis = 1 ), df2[\"ECO2\"], n_ex,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqPKZzGV8Iu"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "use_hsic = False \n",
        "delta = 0.05\n",
        "\n",
        "num_s = np.sum(n_ex)\n",
        "\n",
        "num_predictors = train_x.shape[1]\n",
        "best_subset = np.array([])\n",
        "best_subset_acc = np.array([])\n",
        "best_mse_overall = 1e10\n",
        "\n",
        "already_acc = False\n",
        "\n",
        "selected = np.zeros(num_predictors)\n",
        "accepted_subset = None\n",
        "\n",
        "all_sets, all_pvals = [], []\n",
        "\n",
        "\n",
        "n_iters = 10*num_predictors\n",
        "stay = 1\n",
        "\n",
        "pow_2 = np.array([2**i for i in np.arange(num_predictors)])\n",
        "\n",
        "ind = 0\n",
        "prev_stat = 0\n",
        "\n",
        "bins = []\n",
        "\n",
        "#Get numbers for the mean\n",
        "\n",
        "pred = np.mean(train_y)\n",
        "mse_current = np.mean((pred - valid_y) ** 2)\n",
        "residual = valid_y - pred\n",
        " #   residTup = np.array(residTup).flatten()\n",
        "residTup = levene_pval(residual,n_ex_valid,n_ex_valid.size)\n",
        "\n",
        "levene = sp.stats.levene(*residTup)\n",
        "\n",
        "all_sets.append(np.array([]))\n",
        "all_pvals.append(levene[1])\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if all_pvals[-1]>alpha:\n",
        "  accepted_subset = np.array([])\n",
        "\n",
        "while (stay==1):\n",
        "    \n",
        "    pvals_a = np.zeros(num_predictors)\n",
        "    statistic_a = 1e10 * np.ones(num_predictors)\n",
        "    mse_a = np.zeros(num_predictors)\n",
        "    \n",
        "    for p in range(num_predictors):\n",
        "        current_subset = np.sort(np.where(selected == 1)[0])\n",
        "        regr = linear_model.LinearRegression()\n",
        "        \n",
        "        if selected[p]==0:\n",
        "            subset_add = np.append(current_subset, p).astype(int)\n",
        "            regr.fit(train_x[:,subset_add], train_y.flatten())\n",
        "            \n",
        "            pred = regr.predict(valid_x[:,subset_add])[:,np.newaxis]\n",
        "            mse_current = np.mean((pred - valid_y)**2)\n",
        "            residual = (np.array(valid_y).flatten() - np.array(pred).flatten())\n",
        "            \n",
        "            residTup = levene_pval(residual,n_ex_valid,\n",
        "                                                 n_ex_valid.size)\n",
        "            # print(residual.shape)\n",
        "            levene = sp.stats.levene(*residTup)\n",
        "            \n",
        "            pvals_a[p] = levene[1]\n",
        "            statistic_a[p] = levene[0]\n",
        "            mse_a[p] = mse_current\n",
        "            \n",
        "            all_sets.append(subset_add)\n",
        "            all_pvals.append(levene[1])\n",
        "        \n",
        "        if selected[p] == 1:\n",
        "            acc_rem = np.copy(selected)\n",
        "            acc_rem[p] = 0\n",
        "            \n",
        "            subset_rem = np.sort(np.where(acc_rem == 1)[0])\n",
        "            \n",
        "            if subset_rem.size ==0: continue\n",
        "            \n",
        "            regr = linear_model.LinearRegression()\n",
        "            regr.fit(train_x[:,subset_rem], train_y.flatten())\n",
        "            \n",
        "            pred = regr.predict(valid_x[:,subset_rem])[:,np.newaxis]\n",
        "            mse_current = np.mean((pred - valid_y)**2)\n",
        "            residual = (np.array(valid_y).flatten() - np.array(pred).flatten())\n",
        "            \n",
        "            residTup = levene_pval(residual,n_ex_valid, \n",
        "                                                 n_ex_valid.size)\n",
        "            levene = sp.stats.levene(*residTup)\n",
        "            \n",
        "            pvals_a[p] = levene[1]\n",
        "            statistic_a[p] = levene[0]\n",
        "            mse_a[p] = mse_current\n",
        "            \n",
        "            all_sets.append(subset_rem)\n",
        "            all_pvals.append(levene[1])\n",
        "    \n",
        "    accepted = np.where(pvals_a > alpha)\n",
        "    \n",
        "    if accepted[0].size>0:\n",
        "        best_mse = np.amin(mse_a[np.where(pvals_a > alpha)])\n",
        "        already_acc = True\n",
        "        \n",
        "        selected[np.where(mse_a == best_mse)] = \\\n",
        "          (selected[np.where(mse_a == best_mse)] + 1) % 2\n",
        "        \n",
        "        accepted_subset = np.sort(np.where(selected == 1)[0])\n",
        "        binary = np.sum(pow_2 * selected)\n",
        "        \n",
        "        if (bins==binary).any():\n",
        "            stay = 0\n",
        "        bins.append(binary)\n",
        "    else:\n",
        "        best_pval_arg = np.argmin(statistic_a)\n",
        "        \n",
        "        selected[best_pval_arg] = (selected[best_pval_arg] + 1) % 2\n",
        "        binary = np.sum(pow_2 * selected)\n",
        "        \n",
        "        if (bins==binary).any():\n",
        "            stay = 0\n",
        "        bins.append(binary)\n",
        "    \n",
        "    if ind>n_iters:\n",
        "        stay = 0\n",
        "    ind += 1\n",
        "\n",
        "\n",
        "if accepted_subset is None:\n",
        "  all_pvals = np.array(all_pvals).flatten()\n",
        "  \n",
        "  max_pvals = np.argsort(all_pvals)[-1]\n",
        "  accepted_subset = np.sort(all_sets[max_pvals])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpJvlTseV8Lz",
        "outputId": "3e9d38c7-09cf-4e76-8615-5583950b6d17"
      },
      "source": [
        "print(accepted_subset)\n",
        "print(accepted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9]\n",
            "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17,\n",
            "       18, 19, 20]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "ccj8S6fxV8Na",
        "outputId": "76d7b5a6-76bf-4676-b50a-f716dbaf4d7c"
      },
      "source": [
        "df2.drop(['ECO2'], axis=1)\n",
        "# df2.drop(['ACO2'], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PMB</th>\n",
              "      <th>INT</th>\n",
              "      <th>DISC</th>\n",
              "      <th>KINK</th>\n",
              "      <th>MVS</th>\n",
              "      <th>FIO2</th>\n",
              "      <th>APL</th>\n",
              "      <th>ANES</th>\n",
              "      <th>SHNT</th>\n",
              "      <th>PAP</th>\n",
              "      <th>TPR</th>\n",
              "      <th>VMCH</th>\n",
              "      <th>VTUB</th>\n",
              "      <th>PRSS</th>\n",
              "      <th>VLNG</th>\n",
              "      <th>VALV</th>\n",
              "      <th>MINV</th>\n",
              "      <th>ACO2</th>\n",
              "      <th>PVS</th>\n",
              "      <th>SAO2</th>\n",
              "      <th>CCHL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.634172</td>\n",
              "      <td>2.092500</td>\n",
              "      <td>-0.820810</td>\n",
              "      <td>-1.655160</td>\n",
              "      <td>0.023484</td>\n",
              "      <td>-1.173725</td>\n",
              "      <td>-1.367037</td>\n",
              "      <td>-0.865120</td>\n",
              "      <td>2.892519</td>\n",
              "      <td>38.734281</td>\n",
              "      <td>-2.761450</td>\n",
              "      <td>0.648474</td>\n",
              "      <td>-3.203860</td>\n",
              "      <td>-14.032993</td>\n",
              "      <td>-14.178243</td>\n",
              "      <td>-73.866562</td>\n",
              "      <td>-8.245791</td>\n",
              "      <td>-738.570604</td>\n",
              "      <td>150.291648</td>\n",
              "      <td>78.226727</td>\n",
              "      <td>-5606.616034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.407365</td>\n",
              "      <td>2.065089</td>\n",
              "      <td>-0.897676</td>\n",
              "      <td>-1.219134</td>\n",
              "      <td>-0.099184</td>\n",
              "      <td>-0.968851</td>\n",
              "      <td>-2.407337</td>\n",
              "      <td>-1.043939</td>\n",
              "      <td>2.896333</td>\n",
              "      <td>37.207914</td>\n",
              "      <td>-4.739510</td>\n",
              "      <td>0.755557</td>\n",
              "      <td>-3.883371</td>\n",
              "      <td>-12.419035</td>\n",
              "      <td>-12.773126</td>\n",
              "      <td>-65.575416</td>\n",
              "      <td>-6.760452</td>\n",
              "      <td>-655.917825</td>\n",
              "      <td>134.135415</td>\n",
              "      <td>68.908152</td>\n",
              "      <td>-4989.128290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.707179</td>\n",
              "      <td>2.550885</td>\n",
              "      <td>-0.512960</td>\n",
              "      <td>-0.849870</td>\n",
              "      <td>-0.265847</td>\n",
              "      <td>-0.794231</td>\n",
              "      <td>-0.535513</td>\n",
              "      <td>-0.989653</td>\n",
              "      <td>5.662722</td>\n",
              "      <td>38.785668</td>\n",
              "      <td>-0.810877</td>\n",
              "      <td>1.241027</td>\n",
              "      <td>-5.231297</td>\n",
              "      <td>-13.205503</td>\n",
              "      <td>-13.019669</td>\n",
              "      <td>-64.076344</td>\n",
              "      <td>-5.682553</td>\n",
              "      <td>-640.500168</td>\n",
              "      <td>131.640330</td>\n",
              "      <td>70.093936</td>\n",
              "      <td>-4849.138081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.750950</td>\n",
              "      <td>2.240138</td>\n",
              "      <td>-1.344655</td>\n",
              "      <td>-1.957273</td>\n",
              "      <td>-0.104020</td>\n",
              "      <td>-0.863502</td>\n",
              "      <td>-2.021747</td>\n",
              "      <td>-0.922381</td>\n",
              "      <td>3.611668</td>\n",
              "      <td>39.626073</td>\n",
              "      <td>-4.349381</td>\n",
              "      <td>1.441020</td>\n",
              "      <td>-6.378893</td>\n",
              "      <td>-17.544519</td>\n",
              "      <td>-16.945620</td>\n",
              "      <td>-90.503496</td>\n",
              "      <td>-11.313400</td>\n",
              "      <td>-904.509154</td>\n",
              "      <td>187.639977</td>\n",
              "      <td>96.551917</td>\n",
              "      <td>-6864.874160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.983332</td>\n",
              "      <td>2.175081</td>\n",
              "      <td>-0.925939</td>\n",
              "      <td>-1.882428</td>\n",
              "      <td>0.346551</td>\n",
              "      <td>-1.065333</td>\n",
              "      <td>-1.693710</td>\n",
              "      <td>-0.898171</td>\n",
              "      <td>3.048958</td>\n",
              "      <td>39.472825</td>\n",
              "      <td>-3.493447</td>\n",
              "      <td>-1.936263</td>\n",
              "      <td>6.007108</td>\n",
              "      <td>-12.138160</td>\n",
              "      <td>-11.940216</td>\n",
              "      <td>-59.828340</td>\n",
              "      <td>-5.917730</td>\n",
              "      <td>-598.193593</td>\n",
              "      <td>122.654385</td>\n",
              "      <td>64.099181</td>\n",
              "      <td>-4542.361825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>9.524596</td>\n",
              "      <td>2.233616</td>\n",
              "      <td>-0.669905</td>\n",
              "      <td>-0.808229</td>\n",
              "      <td>0.042524</td>\n",
              "      <td>-0.953088</td>\n",
              "      <td>-1.668014</td>\n",
              "      <td>-1.114540</td>\n",
              "      <td>3.742804</td>\n",
              "      <td>38.483443</td>\n",
              "      <td>-3.526491</td>\n",
              "      <td>-1.088850</td>\n",
              "      <td>3.204424</td>\n",
              "      <td>-8.018281</td>\n",
              "      <td>-8.938673</td>\n",
              "      <td>-41.193547</td>\n",
              "      <td>-2.521904</td>\n",
              "      <td>-412.137627</td>\n",
              "      <td>83.153174</td>\n",
              "      <td>44.039346</td>\n",
              "      <td>-3135.449345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>10.478096</td>\n",
              "      <td>2.294868</td>\n",
              "      <td>-0.910423</td>\n",
              "      <td>-2.051726</td>\n",
              "      <td>0.197059</td>\n",
              "      <td>-1.212614</td>\n",
              "      <td>-1.540881</td>\n",
              "      <td>-0.842339</td>\n",
              "      <td>2.977651</td>\n",
              "      <td>42.376400</td>\n",
              "      <td>-4.374108</td>\n",
              "      <td>-1.063981</td>\n",
              "      <td>3.193246</td>\n",
              "      <td>-13.632071</td>\n",
              "      <td>-13.675745</td>\n",
              "      <td>-69.785635</td>\n",
              "      <td>-7.381488</td>\n",
              "      <td>-697.701584</td>\n",
              "      <td>141.197799</td>\n",
              "      <td>72.550625</td>\n",
              "      <td>-5307.112625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>9.684723</td>\n",
              "      <td>2.280479</td>\n",
              "      <td>-0.906028</td>\n",
              "      <td>-1.609548</td>\n",
              "      <td>0.218766</td>\n",
              "      <td>-1.103314</td>\n",
              "      <td>-1.745732</td>\n",
              "      <td>-1.066011</td>\n",
              "      <td>4.090774</td>\n",
              "      <td>38.631212</td>\n",
              "      <td>-3.735848</td>\n",
              "      <td>-0.797485</td>\n",
              "      <td>2.160329</td>\n",
              "      <td>-12.633280</td>\n",
              "      <td>-11.982887</td>\n",
              "      <td>-59.959497</td>\n",
              "      <td>-5.866067</td>\n",
              "      <td>-599.747697</td>\n",
              "      <td>122.479700</td>\n",
              "      <td>64.008062</td>\n",
              "      <td>-4556.678718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>9.821713</td>\n",
              "      <td>2.358176</td>\n",
              "      <td>-1.377481</td>\n",
              "      <td>-1.150242</td>\n",
              "      <td>0.163891</td>\n",
              "      <td>-1.024320</td>\n",
              "      <td>-1.977978</td>\n",
              "      <td>-0.878067</td>\n",
              "      <td>4.599022</td>\n",
              "      <td>39.791838</td>\n",
              "      <td>-4.199573</td>\n",
              "      <td>-0.181273</td>\n",
              "      <td>-0.987667</td>\n",
              "      <td>-11.537396</td>\n",
              "      <td>-11.954768</td>\n",
              "      <td>-58.417405</td>\n",
              "      <td>-5.129136</td>\n",
              "      <td>-584.008670</td>\n",
              "      <td>120.047792</td>\n",
              "      <td>64.695906</td>\n",
              "      <td>-4429.359071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>10.008516</td>\n",
              "      <td>2.330766</td>\n",
              "      <td>-1.083364</td>\n",
              "      <td>-0.957241</td>\n",
              "      <td>0.265459</td>\n",
              "      <td>-1.088656</td>\n",
              "      <td>-1.966203</td>\n",
              "      <td>-0.830767</td>\n",
              "      <td>3.527820</td>\n",
              "      <td>41.255753</td>\n",
              "      <td>-3.655720</td>\n",
              "      <td>-0.861457</td>\n",
              "      <td>1.916246</td>\n",
              "      <td>-9.990266</td>\n",
              "      <td>-10.289940</td>\n",
              "      <td>-49.011955</td>\n",
              "      <td>-3.588693</td>\n",
              "      <td>-489.764341</td>\n",
              "      <td>99.864790</td>\n",
              "      <td>52.375883</td>\n",
              "      <td>-3721.946736</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           PMB       INT      DISC  ...         PVS       SAO2         CCHL\n",
              "0     9.634172  2.092500 -0.820810  ...  150.291648  78.226727 -5606.616034\n",
              "1     9.407365  2.065089 -0.897676  ...  134.135415  68.908152 -4989.128290\n",
              "2     9.707179  2.550885 -0.512960  ...  131.640330  70.093936 -4849.138081\n",
              "3     9.750950  2.240138 -1.344655  ...  187.639977  96.551917 -6864.874160\n",
              "4     9.983332  2.175081 -0.925939  ...  122.654385  64.099181 -4542.361825\n",
              "..         ...       ...       ...  ...         ...        ...          ...\n",
              "995   9.524596  2.233616 -0.669905  ...   83.153174  44.039346 -3135.449345\n",
              "996  10.478096  2.294868 -0.910423  ...  141.197799  72.550625 -5307.112625\n",
              "997   9.684723  2.280479 -0.906028  ...  122.479700  64.008062 -4556.678718\n",
              "998   9.821713  2.358176 -1.377481  ...  120.047792  64.695906 -4429.359071\n",
              "999  10.008516  2.330766 -1.083364  ...   99.864790  52.375883 -3721.946736\n",
              "\n",
              "[2000 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWVMEX1WnGn"
      },
      "source": [
        "Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt_1phVZV8Rz"
      },
      "source": [
        "# ALARM \n",
        "# # Training data 1 domain\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_train1.csv\")\n",
        "\n",
        "\n",
        "# # # TRAINING + SOFT INT DATA\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_train1.csv\")\n",
        "\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_softINT2.csv\")\n",
        "\n",
        "df2 = df2.append(df)\n",
        "df = df2\n",
        "\n",
        "# # ECO2 TARGET\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df['ANES'], df['ECO2'],test_size=0.2)\n",
        "X_train,X_test,y_train,y_test = train_test_split(df['PAP'], df['ECO2'],test_size=0.2)\n",
        "\n",
        "X_train = X_train.values.reshape(-1,1)\n",
        "X_test = X_test.values.reshape(-1,1)\n",
        "\n",
        "# # ACO2 TARGET\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df['ANES'], df['ACO2'],test_size=0.2)\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df['PAP'], df['ACO2'],test_size=0.2)\n",
        "\n",
        "# X_train = X_train.values.reshape(-1,1)\n",
        "# X_test = X_test.values.reshape(-1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zmBYthtV8TX"
      },
      "source": [
        "# ALARM TESTING DATA\n",
        "\n",
        "# ACO2 TARGET\n",
        "\n",
        "# # Testing Data 8 -> Soft Int2\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_softINT2.csv\")\n",
        "\n",
        "# # X_test = df['ANES']\n",
        "# X_test = df['PAP']\n",
        "# X_test = X_test.values.reshape(-1,1)\n",
        "\n",
        "# y_test = df['ACO2']\n",
        "\n",
        "# # Testing Data 9 -> Hard Int2\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_hardINT2.csv\")\n",
        "\n",
        "# # X_test = df['ANES']\n",
        "# X_test = df['PAP']\n",
        "# X_test = X_test.values.reshape(-1,1)\n",
        "\n",
        "# y_test = df['ACO2']\n",
        "\n",
        "# ECO2 TARGET\n",
        "\n",
        "# # Testing Data 8 -> Soft Int2\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_softINT2.csv\")\n",
        "\n",
        "# # X_test = df['ANES']\n",
        "# X_test = df['PAP']\n",
        "# X_test = X_test.values.reshape(-1,1)\n",
        "\n",
        "# y_test = df['ECO2']\n",
        "\n",
        "# Testing Data 9 -> Hard Int2\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Alarm_Reg_hardINT2.csv\")\n",
        "\n",
        "# X_test = df['ANES']\n",
        "X_test = df['PAP']\n",
        "X_test = X_test.values.reshape(-1,1)\n",
        "\n",
        "y_test = df['ECO2']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxUBblvLV8YT",
        "outputId": "aeb6a67b-2e7d-418e-d54f-808697146127"
      },
      "source": [
        "# Adaboost train\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
        "\n",
        "regressor1 = AdaBoostRegressor(n_estimators=1000, random_state=49)\n",
        "regressor1.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
              "                  n_estimators=1000, random_state=49)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7f_FViVV8Zy",
        "outputId": "b2b2ce20-62c5-4fb8-96f8-09f846a5c6c7"
      },
      "source": [
        "# Adaboost Testing\n",
        "y_pred = regressor1.predict(X_test) \n",
        "# Calculate the absolute errors\n",
        "errors = abs(y_pred - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "evs = explained_variance_score(y_test, y_pred )\n",
        "print(\"Mean squared error =\", round(mse, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 1517.35\n",
            "Mean squared error = 3269846.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0AqoBvFV8ek",
        "outputId": "9da9e441-9009-4946-995c-994727672434"
      },
      "source": [
        "# RF Regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
        "\n",
        "regressor = RandomForestRegressor(n_estimators = 1000,random_state=49)\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
              "                      random_state=49, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSbZgE8lV8gI",
        "outputId": "afb576c2-b7fa-4a57-c2b4-9be6791adc6b"
      },
      "source": [
        "# RF Testing\n",
        "y_pred = regressor.predict(X_test) \n",
        "# Calculate the absolute errors\n",
        "errors = abs(y_pred - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "evs = explained_variance_score(y_test, y_pred )\n",
        "print(\"Mean squared error =\", round(mse, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 1667.98\n",
            "Mean squared error = 4081827.82\n"
          ]
        }
      ]
    }
  ]
}